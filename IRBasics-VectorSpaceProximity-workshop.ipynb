{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# 🛠️ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## 🔍 Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed code—just like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data — such as AI agents.\n",
    "\n",
    "### 👥 Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## 🔧 Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* – Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* – Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Documents loaded: 20\n",
      "✅ Unique vocabulary size: 6866\n",
      "🎯 Requirement satisfied: You can move to the next step!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 📁 Path to the folder where the extracted files are stored\n",
    "folder_path = 'docs/'  # Replace with the path where you extract the ZIP\n",
    "\n",
    "# 🔄 Step 1: Load all .txt documents from folder\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# 🧹 Step 2: Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation/special characters\n",
    "    return text.lower()\n",
    "\n",
    "# 📏 Step 3: Get vocabulary size\n",
    "def get_vocabulary_size(docs):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(docs)\n",
    "    return len(vectorizer.vocabulary_)\n",
    "\n",
    "# 🚀 Run the pipeline\n",
    "documents = load_documents(\"./data/\")\n",
    "cleaned_docs = [clean_text(doc) for doc in documents]\n",
    "vocab_size = get_vocabulary_size(cleaned_docs)\n",
    "\n",
    "# 📊 Output\n",
    "print(f\"✅ Documents loaded: {len(documents)}\")\n",
    "print(f\"✅ Unique vocabulary size: {vocab_size}\")\n",
    "\n",
    "# 💬 Optional check\n",
    "if len(documents) >= 20 and vocab_size >= 2000:\n",
    "    print(\"🎯 Requirement satisfied: You can move to the next step!\")\n",
    "else:\n",
    "    print(\"⚠️ Requirement NOT met — consider using longer/more diverse documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## ✂️ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 tokens: ['usually', 'many', 'skin', 'finish', 'attorney', 'early', 'save', 'boy', 'in', 'store', 'thousand', 'pick', 'clear', 'today', 'face', 'far', 'system', 'star', 'stop', 'summer']\n",
      "Document 2 tokens: ['billion', 'trip', 'stand', 'stage', 'world', 'question', 'people', 'kid', 'price', 'determine', 'eight', 'join', 'whatever', 'friend', 'already', 'yet', 'fall', 'recent', 'it', 'account']\n",
      "Document 3 tokens: ['director', 'century', 'weight', 'statement', 'give', 'various', 'hot', 'similar', 'same', 'act', 'out', 'these', 'land', 'glass', 'three', 'world', 'either', 'mind', 'far', 'nice']\n",
      "Document 4 tokens: ['anyone', 'letter', 'particular', 'like', 'wind', 'whole', 'laugh', 'trip', 'room', 'keep', 'claim', 'ball', 'require', 'worker', 'standard', 'foreign', 'democratic', 'collection', 'skill', 'close']\n",
      "Document 5 tokens: ['best', 'there', 'prevent', 'option', 'among', 'candidate', 'raise', 'shake', 'without', 'customer', 'dog', 'religious', 'congress', 'per', 'dream', 'stuff', 'stock', 'perform', 'relate', 'team']\n",
      "Document 6 tokens: ['glass', 'enter', 'piece', 'third', 'put', 'deal', 'get', 'way', 'which', 'great', 'think', 'well', 'eight', 'lay', 'company', 'receive', 'catch', 'onto', 'special', 'voice']\n",
      "Document 7 tokens: ['crime', 'stage', 'grow', 'fight', 'best', 'pm', 'miss', 'good', 'sing', 'step', 'whose', 'ago', 'manage', 'this', 'available', 'standard', 'window', 'production', 'ball', 'understand']\n",
      "Document 8 tokens: ['north', 'song', 'perform', 'cause', 'region', 'individual', 'number', 'tend', 'stay', 'time', 'apply', 'indicate', 'party', 'mission', 'responsibility', 'wonder', 'six', 'among', 'individual', 'still']\n",
      "Document 9 tokens: ['window', 'way', 'on', 'result', 'make', 'according', 'indeed', 'step', 'family', 'executive', 'determine', 'medical', 'thing', 'skin', 'pass', 'nothing', 'reveal', 'should', 'general', 'glass']\n",
      "Document 10 tokens: ['whether', 'three', 'they', 'gun', 'writer', 'must', 'off', 'save', 'capital', 'account', 'care', 'heavy', 'after', 'southern', 'none', 'factor', 'dog', 'chair', 'hot', 'book']\n",
      "Document 11 tokens: ['election', 'sell', 'pretty', 'hot', 'story', 'include', 'left', 'also', 'subject', 'relationship', 'plant', 'goal', 'market', 'history', 'business', 'current', 'account', 'purpose', 'keep', 'trip']\n",
      "Document 12 tokens: ['skill', 'television', 'from', 'attorney', 'physical', 'financial', 'money', 'entire', 'prove', 'coach', 'sport', 'gun', 'federal', 'so', 'edge', 'really', 'article', 'box', 'drug', 'action']\n",
      "Document 13 tokens: ['turn', 'thing', 'player', 'last', 'garden', 'management', 'also', 'mention', 'growth', 'american', 'dog', 'single', 'fund', 'win', 'feeling', 'simply', 'wish', 'entire', 'argue', 'sign']\n",
      "Document 14 tokens: ['walk', 'attention', 'lose', 'even', 'let', 'religious', 'radio', 'oil', 'live', 'fine', 'various', 'message', 'list', 'blood', 'catch', 'one', 'through', 'friend', 'usually', 'whom']\n",
      "Document 15 tokens: ['door', 'until', 'whether', 'occur', 'avoid', 'policy', 'although', 'present', 'suddenly', 'training', 'growth', 'important', 'might', 'wonder', 'animal', 'back', 'well', 'character', 'happy', 'food']\n",
      "Document 16 tokens: ['relationship', 'girl', 'human', 'often', 'particular', 'beat', 'left', 'than', 'condition', 'accept', 'along', 'common', 'what', 'cause', 'generation', 'mouth', 'deep', 'age', 'wind', 'develop']\n",
      "Document 17 tokens: ['read', 'college', 'event', 'next', 'our', 'up', 'spring', 'management', 'source', 'senior', 'person', 'drug', 'move', 'drug', 'speech', 'instead', 'effort', 'inside', 'exist', 'control']\n",
      "Document 18 tokens: ['whether', 'instead', 'red', 'over', 'full', 'raise', 'unit', 'camera', 'challenge', 'season', 'officer', 'lawyer', 'fill', 'role', 'ten', 'these', 'break', 'can', 'of', 'fight']\n",
      "Document 19 tokens: ['save', 'statement', 'really', 'responsibility', 'growth', 'month', 'agency', 'race', 'attack', 'sound', 'clear', 'walk', 'quality', 'range', 'soon', 'yard', 'account', 'employee', 'tax', 'statement']\n",
      "Document 20 tokens: ['raise', 'loss', 'employee', 'occur', 'become', 'professional', 'buy', 'house', 'moment', 'should', 'not', 'art', 'store', 'base', 'executive', 'color', 'provide', 'individual', 'one', 'you']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Load documents from 'docs/' folder\n",
    "def load_documents(folder_path='docs/'):\n",
    "    documents = []\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Example usage: load all docs and tokenize each\n",
    "docs = load_documents('./Data/')\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    tokens = basic_tokenizer(doc)\n",
    "    print(f\"Document {i} tokens: {tokens[:20]}\")  # print first 20 tokens of each doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## 🔁 Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 normalized tokens: ['usual', 'mani', 'skin', 'finish', 'attorney', 'earli', 'save', 'boy', 'store', 'thousand', 'pick', 'clear', 'today', 'face', 'far', 'system', 'star', 'stop', 'summer', 'film']\n",
      "Document 2 normalized tokens: ['billion', 'trip', 'stand', 'stage', 'world', 'question', 'peopl', 'kid', 'price', 'determin', 'eight', 'join', 'whatev', 'friend', 'alreadi', 'yet', 'fall', 'recent', 'account', 'mother']\n",
      "Document 3 normalized tokens: ['director', 'centuri', 'weight', 'statement', 'give', 'variou', 'hot', 'similar', 'act', 'land', 'glass', 'three', 'world', 'either', 'mind', 'far', 'nice', 'manag', 'continu', 'surfac']\n",
      "Document 4 normalized tokens: ['anyon', 'letter', 'particular', 'like', 'wind', 'whole', 'laugh', 'trip', 'room', 'keep', 'claim', 'ball', 'requir', 'worker', 'standard', 'foreign', 'democrat', 'collect', 'skill', 'close']\n",
      "Document 5 normalized tokens: ['best', 'prevent', 'option', 'among', 'candid', 'rais', 'shake', 'without', 'custom', 'dog', 'religi', 'congress', 'per', 'dream', 'stuff', 'stock', 'perform', 'relat', 'team', 'actual']\n",
      "Document 6 normalized tokens: ['glass', 'enter', 'piec', 'third', 'put', 'deal', 'get', 'way', 'great', 'think', 'well', 'eight', 'lay', 'compani', 'receiv', 'catch', 'onto', 'special', 'voic', 'win']\n",
      "Document 7 normalized tokens: ['crime', 'stage', 'grow', 'fight', 'best', 'pm', 'miss', 'good', 'sing', 'step', 'whose', 'ago', 'manag', 'avail', 'standard', 'window', 'product', 'ball', 'understand', 'view']\n",
      "Document 8 normalized tokens: ['north', 'song', 'perform', 'caus', 'region', 'individu', 'number', 'tend', 'stay', 'time', 'appli', 'indic', 'parti', 'mission', 'respons', 'wonder', 'six', 'among', 'individu', 'still']\n",
      "Document 9 normalized tokens: ['window', 'way', 'result', 'make', 'accord', 'inde', 'step', 'famili', 'execut', 'determin', 'medic', 'thing', 'skin', 'pass', 'noth', 'reveal', 'gener', 'glass', 'loss', 'risk']\n",
      "Document 10 normalized tokens: ['whether', 'three', 'gun', 'writer', 'must', 'save', 'capit', 'account', 'care', 'heavi', 'southern', 'none', 'factor', 'dog', 'chair', 'hot', 'book', 'strong', 'factor', 'budget']\n",
      "Document 11 normalized tokens: ['elect', 'sell', 'pretti', 'hot', 'stori', 'includ', 'left', 'also', 'subject', 'relationship', 'plant', 'goal', 'market', 'histori', 'busi', 'current', 'account', 'purpos', 'keep', 'trip']\n",
      "Document 12 normalized tokens: ['skill', 'televis', 'attorney', 'physic', 'financi', 'money', 'entir', 'prove', 'coach', 'sport', 'gun', 'feder', 'edg', 'realli', 'articl', 'box', 'drug', 'action', 'go', 'usual']\n",
      "Document 13 normalized tokens: ['turn', 'thing', 'player', 'last', 'garden', 'manag', 'also', 'mention', 'growth', 'american', 'dog', 'singl', 'fund', 'win', 'feel', 'simpli', 'wish', 'entir', 'argu', 'sign']\n",
      "Document 14 normalized tokens: ['walk', 'attent', 'lose', 'even', 'let', 'religi', 'radio', 'oil', 'live', 'fine', 'variou', 'messag', 'list', 'blood', 'catch', 'one', 'friend', 'usual', 'media', 'guess']\n",
      "Document 15 normalized tokens: ['door', 'whether', 'occur', 'avoid', 'polici', 'although', 'present', 'suddenli', 'train', 'growth', 'import', 'might', 'wonder', 'anim', 'back', 'well', 'charact', 'happi', 'food', 'place']\n",
      "Document 16 normalized tokens: ['relationship', 'girl', 'human', 'often', 'particular', 'beat', 'left', 'condit', 'accept', 'along', 'common', 'caus', 'gener', 'mouth', 'deep', 'age', 'wind', 'develop', 'director', 'friend']\n",
      "Document 17 normalized tokens: ['read', 'colleg', 'event', 'next', 'spring', 'manag', 'sourc', 'senior', 'person', 'drug', 'move', 'drug', 'speech', 'instead', 'effort', 'insid', 'exist', 'control', 'degre', 'well']\n",
      "Document 18 normalized tokens: ['whether', 'instead', 'red', 'full', 'rais', 'unit', 'camera', 'challeng', 'season', 'offic', 'lawyer', 'fill', 'role', 'ten', 'break', 'fight', 'administr', 'ask', 'coupl', 'democrat']\n",
      "Document 19 normalized tokens: ['save', 'statement', 'realli', 'respons', 'growth', 'month', 'agenc', 'race', 'attack', 'sound', 'clear', 'walk', 'qualiti', 'rang', 'soon', 'yard', 'account', 'employe', 'tax', 'statement']\n",
      "Document 20 normalized tokens: ['rais', 'loss', 'employe', 'occur', 'becom', 'profession', 'buy', 'hous', 'moment', 'art', 'store', 'base', 'execut', 'color', 'provid', 'individu', 'one', 'movement', 'method', 'behavior']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download stopwords once\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def load_documents(folder_path='./Data/'):\n",
    "    documents = []\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Load and normalize all documents\n",
    "docs = load_documents('./Data/')\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    normalized_tokens = normalize_tokens(doc)\n",
    "    print(f\"Document {i} normalized tokens: {normalized_tokens[:20]}\")  # first 20 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "    ### 🗣 Instructor Talking Point:\n",
    "    > We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "    ### 🔧 Your Task:\n",
    "    - Build the inverted index using a dictionary.\n",
    "    - Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens from inverted index:\n",
      "usual: [0, 2, 6, 8, 11, 12, 13, 16, 18]\n",
      "mani: [0, 1, 2, 3, 9, 12, 13, 16, 18]\n",
      "skin: [0, 1, 3, 4, 6, 7, 8, 10, 12, 15, 16]\n",
      "finish: [0, 3, 5, 11, 13, 17, 19]\n",
      "attorney: [0, 3, 4, 7, 8, 9, 10, 11, 13, 14, 15, 16]\n",
      "earli: [0, 2, 5, 7, 17]\n",
      "save: [0, 2, 3, 4, 5, 7, 8, 9, 12, 13, 18]\n",
      "boy: [0, 1, 2, 3, 4, 8, 9, 10, 14, 15, 19]\n",
      "store: [0, 5, 6, 7, 8, 11, 14, 16, 17, 19]\n",
      "thousand: [0, 2, 3, 5, 8, 10, 12, 13, 19]\n",
      "\n",
      "Documents containing the phrase 'machine learning': [4, 11]\n",
      "\n",
      "Documents containing the phrase 'artificial intelligence': [3, 11, 17]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    filtered_stemmed = [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "    return filtered_stemmed\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    inverted_index = {}\n",
    "    for doc_id, text in enumerate(docs):\n",
    "        tokens = normalize_tokens(text)\n",
    "        seen_in_doc = set()\n",
    "        for pos, token in enumerate(tokens):\n",
    "            # Add positional info for phrase queries\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {}\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token][doc_id] = []\n",
    "            inverted_index[token][doc_id].append(pos)\n",
    "            seen_in_doc.add(token)\n",
    "    return inverted_index\n",
    "\n",
    "def phrase_in_doc(inverted_index, phrase_tokens, doc_id):\n",
    "    positions_lists = []\n",
    "    for token in phrase_tokens:\n",
    "        if token not in inverted_index or doc_id not in inverted_index[token]:\n",
    "            return False\n",
    "        positions_lists.append(inverted_index[token][doc_id])\n",
    "\n",
    "    # Check positions for sequential occurrence of phrase tokens\n",
    "    first_positions = positions_lists[0]\n",
    "    for start_pos in first_positions:\n",
    "        if all((start_pos + offset) in positions_lists[offset] for offset in range(1, len(positions_lists))):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def phrase_search(inverted_index, phrase, docs):\n",
    "    phrase_tokens = normalize_tokens(phrase)\n",
    "    matched_docs = []\n",
    "    for doc_id in range(len(docs)):\n",
    "        if phrase_in_doc(inverted_index, phrase_tokens, doc_id):\n",
    "            matched_docs.append(doc_id)\n",
    "    return matched_docs\n",
    "\n",
    "def load_documents(folder_path='./Data/'):\n",
    "    documents = []\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                documents.append(f.read())\n",
    "    return documents\n",
    "\n",
    "# ---- Main ----\n",
    "docs = load_documents('./Data/')\n",
    "\n",
    "inverted_index = build_inverted_index(docs)\n",
    "\n",
    "print(\"Sample tokens from inverted index:\")\n",
    "for token, postings in list(inverted_index.items())[:10]:\n",
    "    print(f\"{token}: {list(postings.keys())}\")\n",
    "\n",
    "# Test phrase queries\n",
    "phrases = [\"machine learning\", \"artificial intelligence\"]\n",
    "\n",
    "for phrase in phrases:\n",
    "    matched = phrase_search(inverted_index, phrase, docs)\n",
    "    print(f\"\\nDocuments containing the phrase '{phrase}': {matched}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## 🧪 Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents containing phrase 'machine learning': [0, 1, 2]\n",
      "Documents containing phrase 'deep learning': [1]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    inverted_index = {}\n",
    "    for doc_id, text in enumerate(docs):\n",
    "        tokens = basic_tokenizer(text)\n",
    "        for pos, token in enumerate(tokens):\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {}\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token][doc_id] = []\n",
    "            inverted_index[token][doc_id].append(pos)\n",
    "    return inverted_index\n",
    "\n",
    "def phrase_in_doc(inverted_index, phrase_tokens, doc_id):\n",
    "    positions_lists = []\n",
    "    for token in phrase_tokens:\n",
    "        if token not in inverted_index or doc_id not in inverted_index[token]:\n",
    "            return False\n",
    "        positions_lists.append(inverted_index[token][doc_id])\n",
    "    first_positions = positions_lists[0]\n",
    "    for start_pos in first_positions:\n",
    "        if all((start_pos + offset) in positions_lists[offset] for offset in range(1, len(positions_lists))):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def phrase_search(inverted_index, phrase, docs):\n",
    "    phrase_tokens = basic_tokenizer(phrase)\n",
    "    matched_docs = []\n",
    "    for doc_id in range(len(docs)):\n",
    "        if phrase_in_doc(inverted_index, phrase_tokens, doc_id):\n",
    "            matched_docs.append(doc_id)\n",
    "    return matched_docs\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Artificial intelligence includes machine learning.\",\n",
    "    \"Learning about machine algorithms.\"\n",
    "]\n",
    "\n",
    "# Build index\n",
    "index = build_inverted_index(docs)\n",
    "\n",
    "# Phrase queries\n",
    "phrases = [\"machine learning\", \"deep learning\"]\n",
    "\n",
    "for phrase in phrases:\n",
    "    matched = phrase_search(index, phrase, docs)\n",
    "    print(f\"Documents containing phrase '{phrase}': {matched}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b479e93",
   "metadata": {},
   "source": [
    "# 🧠 NLP Foundations Workshop: From Preprocessing to tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c04c3",
   "metadata": {},
   "source": [
    "\n",
    "**Duration**: 90 minutes  \n",
    "**Team Size**: 3 students  \n",
    "**Objective**: Build an NLP pipeline from scratch to implement and test six foundational concepts in Natural Language Processing in preparation for Vector Space Models and Cosine Similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e0c14",
   "metadata": {},
   "source": [
    "## Step 1: Presenting the Six Core NLP Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7f198",
   "metadata": {},
   "source": [
    "### 🔹 Term-Document Incidence Matrix\n",
    "\n",
    "The **Term-Document Incidence Matrix** is a binary matrix that shows whether a term $t$ appears in a document $d$.\n",
    "\n",
    "- Rows represent terms in the vocabulary  \n",
    "- Columns represent documents in the corpus  \n",
    "- Each entry $w_{t,d}$ is defined as:\n",
    "\n",
    "$$\n",
    "w_{t,d} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } t \\in d \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This is a **binary representation** — it only records the **presence or absence** of a term, not how many times it appears.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why Use It?\n",
    "\n",
    "- It’s the **simplest form** of representing document contents using structured data.\n",
    "- Useful for:\n",
    "  - Boolean search and keyword filters\n",
    "  - Document classification based on keyword sets\n",
    "  - Building foundational **retrieval systems**\n",
    "- Helps in detecting whether **all query terms exist** in a document (e.g., phrase queries or \"AND\" operations)\n",
    "\n",
    "---\n",
    "\n",
    "#### 📘 Example\n",
    "\n",
    "Suppose we have 3 documents:\n",
    "\n",
    "- **Doc1**: \"machine learning is fun\"  \n",
    "- **Doc2**: \"deep learning is powerful\"  \n",
    "- **Doc3**: \"machine learning and deep models\"\n",
    "\n",
    "The vocabulary extracted from all three is:\n",
    "\n",
    "**Vocabulary** = {machine, learning, is, fun, deep, powerful, and, models}\n",
    "\n",
    "The Term-Document Incidence Matrix would look like:\n",
    "\n",
    "| Term       | Doc1 | Doc2 | Doc3 |\n",
    "|------------|------|------|------|\n",
    "| machine    | 1    | 0    | 1    |\n",
    "| learning   | 1    | 1    | 1    |\n",
    "| is         | 1    | 1    | 0    |\n",
    "| fun        | 1    | 0    | 0    |\n",
    "| deep       | 0    | 1    | 1    |\n",
    "| powerful   | 0    | 1    | 0    |\n",
    "| and        | 0    | 0    | 1    |\n",
    "| models     | 0    | 0    | 1    |\n",
    "\n",
    "For example:\n",
    "- $w_{\\text{machine}, \\text{Doc1}} = 1$ → \"machine\" is in Doc1\n",
    "- $w_{\\text{powerful}, \\text{Doc1}} = 0$ → \"powerful\" is not in Doc1\n",
    "\n",
    "This matrix is particularly helpful when implementing **Boolean retrieval systems** and **phrase matching**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14ef537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Term-Document Incidence Matrix:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "and",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "deep",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fun",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "machine",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "models",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "powerful",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e65192e1-0c96-4a8c-82da-18fcbac326c9",
       "rows": [
        [
         "Doc1",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0"
        ],
        [
         "Doc2",
         "0",
         "1",
         "0",
         "1",
         "1",
         "0",
         "0",
         "1"
        ],
        [
         "Doc3",
         "1",
         "1",
         "0",
         "0",
         "1",
         "1",
         "1",
         "0"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>deep</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>powerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      and  deep  fun  is  learning  machine  models  powerful\n",
       "Doc1    0     0    1   1         1        1       0         0\n",
       "Doc2    0     1    0   1         1        0       0         1\n",
       "Doc3    1     1    0   0         1        1       1         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📘 Example: Term-Document Incidence Matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus from the Markdown example\n",
    "docs = [\n",
    "    \"machine learning is fun\",          # Doc1\n",
    "    \"deep learning is powerful\",        # Doc2\n",
    "    \"machine learning and deep models\"  # Doc3\n",
    "]\n",
    "\n",
    "# Use binary=True to indicate presence/absence (1 or 0)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Create a labeled DataFrame\n",
    "incidence_matrix = pd.DataFrame(X.toarray(),\n",
    "                                index=[\"Doc1\", \"Doc2\", \"Doc3\"],\n",
    "                                columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the incidence matrix\n",
    "print(\"🔎 Term-Document Incidence Matrix:\")\n",
    "display(incidence_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d030a",
   "metadata": {},
   "source": [
    "## Talking Points:\n",
    "\n",
    "Each row represents a `.txt` file, and each column is a term. The matrix tells us whether a word appears (1) or not (0) in a document — useful for Boolean search systems. Try selecting two keywords and checking which documents contain both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bc6a1",
   "metadata": {},
   "source": [
    "🗣️ **Instructor Talking Point**: This code demonstrates how the presence or absence of a term in a document is encoded as a binary matrix — foundational for Boolean retrieval. Explain this with respect to a future AI agent (chatbot) builds context.\n",
    "<br/>\n",
    "<br/>\n",
    "🧠 **Student Talking Point**: Add a phrase query (e.g., 'machine learning') and explain your reasoning as to how you would check if both terms occur in a single document using this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e547ea",
   "metadata": {},
   "source": [
    "### 🔹 Term Frequency (TF)\n",
    "\n",
    "**Term Frequency (TF)** measures how frequently a term $t$ appears in a document $d$.\n",
    "\n",
    "$$\n",
    "tf_{t,d} = f_{t,d}\n",
    "$$\n",
    "\n",
    "Where $f_{t,d}$ is the raw count of term $t$ in document $d$.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why Use It?\n",
    "\n",
    "- TF reflects the importance of a word **within a specific document**.\n",
    "- A higher TF means the term is likely central to the topic of that document.\n",
    "- It's used as the **first step** in vectorizing text for machine learning models like classification, clustering, or information retrieval.\n",
    "\n",
    "TF is most effective when combined with **IDF** (Inverse Document Frequency) to balance against very common terms across the corpus.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📘 Example\n",
    "\n",
    "Let’s say we have this document:\n",
    "\n",
    "> **Doc1**: `\"machine learning is fun and machine learning is useful\"`\n",
    "\n",
    "Calculate raw term counts:\n",
    "\n",
    "| Term     | Raw TF $(f_{t,d})$ |\n",
    "|----------|--------------------|\n",
    "| machine  | 2                  |\n",
    "| learning | 2                  |\n",
    "| is       | 2                  |\n",
    "| fun      | 1                  |\n",
    "| and      | 1                  |\n",
    "| useful   | 1                  |\n",
    "\n",
    "If normalized (total of 9 words):\n",
    "\n",
    "- $tf(\\text{\"machine\"}, \\text{Doc1}) = \\frac{2}{9} \\approx 0.22$\n",
    "- $tf(\\text{\"learning\"}, \\text{Doc1}) = \\frac{2}{9} \\approx 0.22$\n",
    "\n",
    "This simple frequency can then be used as input into models such as **TF-IDF**, which adjusts these values based on how rare the words are across multiple documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b512919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Raw Term Frequencies:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Raw TF",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "07fc2a65-d5b6-4217-9e5c-c3691ff56991",
       "rows": [
        [
         "0",
         "machine",
         "2"
        ],
        [
         "1",
         "learning",
         "2"
        ],
        [
         "2",
         "is",
         "2"
        ],
        [
         "3",
         "fun",
         "1"
        ],
        [
         "4",
         "and",
         "1"
        ],
        [
         "5",
         "useful",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Raw TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>useful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term  Raw TF\n",
       "0   machine       2\n",
       "1  learning       2\n",
       "2        is       2\n",
       "3       fun       1\n",
       "4       and       1\n",
       "5    useful       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Normalized Term Frequencies:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TF (Normalized)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "5831787a-3cc6-4e16-ba6f-b9bac7b70199",
       "rows": [
        [
         "0",
         "machine",
         "0.2222222222222222"
        ],
        [
         "1",
         "learning",
         "0.2222222222222222"
        ],
        [
         "2",
         "is",
         "0.2222222222222222"
        ],
        [
         "3",
         "fun",
         "0.1111111111111111"
        ],
        [
         "4",
         "and",
         "0.1111111111111111"
        ],
        [
         "5",
         "useful",
         "0.1111111111111111"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>TF (Normalized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fun</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>useful</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term  TF (Normalized)\n",
       "0   machine         0.222222\n",
       "1  learning         0.222222\n",
       "2        is         0.222222\n",
       "3       fun         0.111111\n",
       "4       and         0.111111\n",
       "5    useful         0.111111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📘 Example: Term Frequency (TF)\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Sample document\n",
    "doc1 = \"machine learning is fun and machine learning is useful\"\n",
    "\n",
    "# Tokenize the document (simple lowercase + split)\n",
    "tokens = doc1.lower().split()\n",
    "\n",
    "# Count term frequencies\n",
    "tf_raw = Counter(tokens)\n",
    "\n",
    "# Total number of words\n",
    "total_terms = len(tokens)\n",
    "\n",
    "# Compute normalized TF\n",
    "tf_normalized = {term: count / total_terms for term, count in tf_raw.items()}\n",
    "\n",
    "# Display results\n",
    "print(\"🔢 Raw Term Frequencies:\")\n",
    "display(pd.DataFrame(tf_raw.items(), columns=[\"Term\", \"Raw TF\"]))\n",
    "\n",
    "print(\"\\n📏 Normalized Term Frequencies:\")\n",
    "display(pd.DataFrame(tf_normalized.items(), columns=[\"Term\", \"TF (Normalized)\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab50d4",
   "metadata": {},
   "source": [
    "## Talking Points:\n",
    "\n",
    "This shows which terms are most frequent in the selected document. Words with higher normalized TF values are more likely to represent the main topics in that file. Try comparing these results across multiple documents to identify topic-specific keywords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5e206",
   "metadata": {},
   "source": [
    "🗣️ **Instructor Talking Point**: \"Here we count how often each term appears in a single document and normalize it. This is the simplest way to represent word importance within a document. Explain this with respect to a future AI agent (chatbot) builds  builds context.\n",
    "<br/>\n",
    "<br/>\n",
    "🧠 **Student Talking Point**: \"Use this TF output to compare with another document. Which terms are likely to be most important in Doc1 based on their normalized TF? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f347108",
   "metadata": {},
   "source": [
    "### 🔹 Log Frequency Weight\n",
    "\n",
    "To reduce the impact of very frequent terms, **log frequency weighting** is applied.\n",
    "\n",
    "$$\n",
    "w_{t,d} =\n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(f_{t,d}) & \\text{if } f_{t,d} > 0 \\\\\n",
    "0 & \\text{if } f_{t,d} = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This transformation reduces the skew caused by terms that appear many times in a document. Instead of allowing their raw frequency to dominate, we scale their contribution **logarithmically**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why Use It?\n",
    "\n",
    "- Frequent terms are not always the most **important** terms.\n",
    "- Log scaling ensures that:\n",
    "  - Words with a raw count of 1 are preserved ($1 + \\\\log_{10}(1) = 1$),\n",
    "  - But words with very high counts (e.g., 1000) don’t dominate the document vector.\n",
    "\n",
    "This helps **normalize the influence** of repetitive terms and improve the **numerical stability** of document representations in models.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📘 Example\n",
    "\n",
    "Let’s say we have a document with the following raw term counts:\n",
    "\n",
    "| Term     | Raw TF $f_{t,d}$ | Log Frequency Weight $w_{t,d}$ |\n",
    "|----------|------------------|-------------------------------|\n",
    "| machine  | 1                | $1 + \\\\log_{10}(1) = 1$        |\n",
    "| learning | 3                | $1 + \\\\log_{10}(3) \\approx 1.477$ |\n",
    "| data     | 10               | $1 + \\\\log_{10}(10) = 2$       |\n",
    "\n",
    "So even though \"data\" appears 10 times, its log-weighted value is **just 2**, making it more comparable to less frequent but potentially more meaningful terms like \"learning\".\n",
    "\n",
    "This makes log frequency weighting especially useful when preparing inputs for models like **TF-IDF** or **document clustering**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4bcea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Log Frequency Weighting:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Raw TF (f_{t,d})",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Log Weight (w_{t,d})",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a08f5cce-c322-4ba9-b0a5-0b3e2f02bbee",
       "rows": [
        [
         "0",
         "machine",
         "2",
         "1.3010299956639813"
        ],
        [
         "1",
         "learning",
         "4",
         "1.6020599913279625"
        ],
        [
         "2",
         "data",
         "7",
         "1.845098040014257"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Raw TF (f_{t,d})</th>\n",
       "      <th>Log Weight (w_{t,d})</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>2</td>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>4</td>\n",
       "      <td>1.602060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data</td>\n",
       "      <td>7</td>\n",
       "      <td>1.845098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term  Raw TF (f_{t,d})  Log Weight (w_{t,d})\n",
       "0   machine                 2              1.301030\n",
       "1  learning                 4              1.602060\n",
       "2      data                 7              1.845098"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📘 Example: Log Frequency Weighting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Sample document with varying term frequencies\n",
    "doc = \"machine learning data data data learning learning learning machine data data data data\"\n",
    "\n",
    "# Tokenize and count raw term frequencies\n",
    "tokens = doc.lower().split()\n",
    "raw_tf = Counter(tokens)\n",
    "\n",
    "# Compute log frequency weights\n",
    "log_weighted_tf = {\n",
    "    term: 1 + np.log10(freq) if freq > 0 else 0\n",
    "    for term, freq in raw_tf.items()\n",
    "}\n",
    "\n",
    "# Build and display the result as a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Term\": raw_tf.keys(),\n",
    "    \"Raw TF (f_{t,d})\": raw_tf.values(),\n",
    "    \"Log Weight (w_{t,d})\": log_weighted_tf.values()\n",
    "})\n",
    "\n",
    "print(\"📊 Log Frequency Weighting:\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d70328",
   "metadata": {},
   "source": [
    "## Talking Point:\n",
    "\n",
    "This section demonstrates how **logarithmic scaling** is applied to raw term frequencies to reduce the impact of frequently occurring words.\n",
    "\n",
    "In the code, we take the first document, tokenize it, and count how many times each term appears. Instead of treating a word that appears 10 times as 10× more important than a word that appears once, we apply the formula:\n",
    "\n",
    "> **Log Weight =** `1 + log10(frequency)` if frequency > 0, else 0\n",
    "\n",
    "This **compresses large values** and prevents them from dominating the vector space.\n",
    "\n",
    "For example:\n",
    "\n",
    "* A word with frequency = 1 → log weight = **1**\n",
    "* A word with frequency = 10 → log weight ≈ **2**\n",
    "* A word with frequency = 100 → log weight ≈ **3**\n",
    "\n",
    "The resulting DataFrame shows each term with its raw count and corresponding log-weighted value.\n",
    "\n",
    "> **Why it matters**: This technique makes document representations more balanced and ensures rare but meaningful words aren't drowned out by repetitive, less informative ones — an essential step before TF-IDF weighting or document comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278f040",
   "metadata": {},
   "source": [
    "🗣️ **Instructor Talking Point**: Note how 'data' has a high frequency, but its impact is smoothed by log weighting, making it comparable to 'learning'. Explain this with respect to how a future AI agent (chatbot) builds builds context.\n",
    "<br/>\n",
    "<br/>\n",
    "🧠 **Student Talking Point**: Try adjusting the number of times a word appears and observe how the log scale compresses large values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da22b7",
   "metadata": {},
   "source": [
    "### 🔹 Document Frequency (DF)\n",
    "\n",
    "**Document Frequency** is the number of documents in which a term $t$ appears:\n",
    "\n",
    "$$\n",
    "df_t = |\\{ d \\in D : t \\in d \\}|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $df_t$ is the document frequency of term $t$\n",
    "- $D$ is the set of all documents in the corpus\n",
    "- $t \\in d$ means the term $t$ appears in document $d$\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why Use It?\n",
    "\n",
    "- It helps you understand **how common or rare** a word is across the entire document set.\n",
    "- Words with **high DF** (e.g., “the”, “and”) occur in many documents and are often **less informative**.\n",
    "- Words with **low DF** are more likely to be **specific and meaningful** for distinguishing between documents.\n",
    "- DF is a key ingredient in calculating **Inverse Document Frequency (IDF)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📘 Example\n",
    "\n",
    "Suppose you have the following three documents:\n",
    "\n",
    "- **Doc1**: \"machine learning is fun\"  \n",
    "- **Doc2**: \"deep learning is powerful\"  \n",
    "- **Doc3**: \"machine learning and deep models\"\n",
    "\n",
    "Now, let’s compute the Document Frequency:\n",
    "\n",
    "| Term     | Document Frequency ($df_t$) |\n",
    "|----------|-----------------------------|\n",
    "| machine  | 2 (Doc1, Doc3)              |\n",
    "| learning | 3 (Doc1, Doc2, Doc3)        |\n",
    "| deep     | 2 (Doc2, Doc3)              |\n",
    "| models   | 1 (Doc3)                    |\n",
    "\n",
    "The term **\"learning\"** appears in all three documents → **high DF**, which means it’s **less useful for distinguishing** between them.\n",
    "\n",
    "The term **\"models\"** appears in only one document → **low DF**, meaning it could be a **useful keyword** for that specific document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993b6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Document Frequency (DF) Table:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Document Frequency (df_t)",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "378ce4f1-59fb-49ff-8215-72a087225c93",
       "rows": [
        [
         "4",
         "learning",
         "3"
        ],
        [
         "1",
         "deep",
         "2"
        ],
        [
         "5",
         "machine",
         "2"
        ],
        [
         "3",
         "is",
         "2"
        ],
        [
         "2",
         "fun",
         "1"
        ],
        [
         "0",
         "and",
         "1"
        ],
        [
         "6",
         "models",
         "1"
        ],
        [
         "7",
         "powerful",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Document Frequency (df_t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>learning</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deep</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>models</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>powerful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term  Document Frequency (df_t)\n",
       "4  learning                          3\n",
       "1      deep                          2\n",
       "5   machine                          2\n",
       "3        is                          2\n",
       "2       fun                          1\n",
       "0       and                          1\n",
       "6    models                          1\n",
       "7  powerful                          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📘 Example: Document Frequency (DF)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents from Curriculum Learning (4)\n",
    "docs = [\n",
    "    \"machine learning is fun\",          # Doc1\n",
    "    \"deep learning is powerful\",        # Doc2\n",
    "    \"machine learning and deep models\"  # Doc3\n",
    "]\n",
    "\n",
    "# Use CountVectorizer to extract term-document matrix (raw counts)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Get feature names and document-term matrix as array\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Calculate document frequency for each term\n",
    "df_counts = (X_array > 0).sum(axis=0)\n",
    "\n",
    "# Format as a DataFrame\n",
    "df_table = pd.DataFrame({\n",
    "    \"Term\": terms,\n",
    "    \"Document Frequency (df_t)\": df_counts\n",
    "}).sort_values(\"Document Frequency (df_t)\", ascending=False)\n",
    "\n",
    "print(\"📊 Document Frequency (DF) Table:\")\n",
    "display(df_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761efed9",
   "metadata": {},
   "source": [
    "## Talking Point:\n",
    "\n",
    "Document Frequency tells us how many documents a term appears in.\n",
    "\n",
    "- **High DF** terms (like “the”, “is”, “data”) appear in many documents — they are often common and less meaningful.\n",
    "\n",
    "- **Low DF** terms appear in fewer documents and are often more specific and informative for identifying or distinguishing topics.\n",
    "\n",
    "This DF table helps us identify generic vs. unique terms, which is crucial before computing IDF and TF-IDF scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d74152",
   "metadata": {},
   "source": [
    "🗣️ **Instructor Talking Point**: Notice how common terms like 'learning' appear in all documents, while more specific terms like 'fun' or 'models' appear in only one.\n",
    "<br/>\n",
    "<br/>\n",
    "🧠 **Student Talking Point**: Choose a term and explain how its document frequency could affect downstream TF-IDF weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012a299",
   "metadata": {},
   "source": [
    "### 🔹 Inverse Document Frequency (IDF)\n",
    "\n",
    "**Inverse Document Frequency (IDF)** measures how rare or informative a term is across the entire corpus:\n",
    "\n",
    "$$\n",
    "idf_t = \\log_{10} \\left( \\frac{N}{df_t} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the total number of documents in the corpus  \n",
    "- $df_t$ is the number of documents that contain the term $t$\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why Use It?\n",
    "\n",
    "- IDF is used to **downweight common terms** and **upweight rare ones**.\n",
    "- Words like “the”, “and”, or “data” appear frequently and are less helpful in distinguishing documents.\n",
    "- Terms that appear in **fewer documents** are often **more informative** and **discriminative**.\n",
    "- IDF is a core component of **TF-IDF**, a widely used technique in search engines, document classification, and clustering.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📘 Example\n",
    "\n",
    "Let’s say we have **5 documents** total, and the following document frequencies:\n",
    "\n",
    "| Term     | $df_t$ | $idf_t = \\log_{10}(N / df_t)$ |\n",
    "|----------|--------|-------------------------------|\n",
    "| machine  | 3      | $\\log_{10}(5 / 3) \\approx 0.22$ |\n",
    "| entropy  | 1      | $\\log_{10}(5 / 1) = 0.70$       |\n",
    "| the      | 5      | $\\log_{10}(5 / 5) = 0.00$       |\n",
    "\n",
    "- The term **\"entropy\"** appears in only one document, so its IDF is **high** → it’s a **rare and informative term**.\n",
    "- The term **\"the\"** ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4553c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Inverse Document Frequency (IDF) Table:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Document Frequency (df_t)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "IDF (log10(N / df_t))",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "488bb9d4-e290-4554-ab93-a7c9d4d01f39",
       "rows": [
        [
         "0",
         "advanced",
         "1",
         "0.6989700043360189"
        ],
        [
         "1",
         "ai",
         "1",
         "0.6989700043360189"
        ],
        [
         "2",
         "and",
         "1",
         "0.6989700043360189"
        ],
        [
         "3",
         "are",
         "1",
         "0.6989700043360189"
        ],
        [
         "4",
         "deep",
         "1",
         "0.6989700043360189"
        ],
        [
         "5",
         "entropy",
         "1",
         "0.6989700043360189"
        ],
        [
         "6",
         "evolving",
         "1",
         "0.6989700043360189"
        ],
        [
         "11",
         "of",
         "1",
         "0.6989700043360189"
        ],
        [
         "14",
         "science",
         "1",
         "0.6989700043360189"
        ],
        [
         "10",
         "measures",
         "1",
         "0.6989700043360189"
        ],
        [
         "13",
         "randomness",
         "1",
         "0.6989700043360189"
        ],
        [
         "12",
         "powerful",
         "1",
         "0.6989700043360189"
        ],
        [
         "15",
         "the",
         "1",
         "0.6989700043360189"
        ],
        [
         "7",
         "is",
         "2",
         "0.3979400086720376"
        ],
        [
         "9",
         "machine",
         "3",
         "0.2218487496163564"
        ],
        [
         "8",
         "learning",
         "4",
         "0.09691001300805642"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Document Frequency (df_t)</th>\n",
       "      <th>IDF (log10(N / df_t))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advanced</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ai</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>evolving</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>science</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>measures</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>randomness</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>powerful</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>machine</td>\n",
       "      <td>3</td>\n",
       "      <td>0.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>learning</td>\n",
       "      <td>4</td>\n",
       "      <td>0.096910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Term  Document Frequency (df_t)  IDF (log10(N / df_t))\n",
       "0     advanced                          1               0.698970\n",
       "1           ai                          1               0.698970\n",
       "2          and                          1               0.698970\n",
       "3          are                          1               0.698970\n",
       "4         deep                          1               0.698970\n",
       "5      entropy                          1               0.698970\n",
       "6     evolving                          1               0.698970\n",
       "11          of                          1               0.698970\n",
       "14     science                          1               0.698970\n",
       "10    measures                          1               0.698970\n",
       "13  randomness                          1               0.698970\n",
       "12    powerful                          1               0.698970\n",
       "15         the                          1               0.698970\n",
       "7           is                          2               0.397940\n",
       "9      machine                          3               0.221849\n",
       "8     learning                          4               0.096910"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📘 Example: Inverse Document Frequency (IDF)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents (5 total)\n",
    "docs = [\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is advanced\",\n",
    "    \"entropy measures randomness\",\n",
    "    \"machine learning and AI are evolving\",\n",
    "    \"the science of machine learning\"\n",
    "]\n",
    "\n",
    "# Total number of documents\n",
    "N = len(docs)\n",
    "\n",
    "# Use CountVectorizer to get document-term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Compute document frequency for each term\n",
    "df_counts = (X_array > 0).sum(axis=0)\n",
    "\n",
    "# Compute IDF using log base 10\n",
    "idf_values = np.log10(N / df_counts)\n",
    "\n",
    "# Build a DataFrame for display\n",
    "idf_table = pd.DataFrame({\n",
    "    \"Term\": terms,\n",
    "    \"Document Frequency (df_t)\": df_counts,\n",
    "    \"IDF (log10(N / df_t))\": idf_values\n",
    "}).sort_values(\"IDF (log10(N / df_t))\", ascending=False)\n",
    "\n",
    "print(\"📊 Inverse Document Frequency (IDF) Table:\")\n",
    "display(idf_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875d7aa",
   "metadata": {},
   "source": [
    "## Talking Point:\n",
    "\n",
    "Pick one high-IDF term (rare across documents) and one low-IDF term (common across documents) from the table.\n",
    "\n",
    "- The high-IDF term is rare, so it carries more discriminative power — it helps differentiate documents better.\n",
    "\n",
    "- The low-IDF term appears in many documents, making it less useful for distinguishing content since it’s common and generic.\n",
    "\n",
    "This balance helps TF-IDF focus on terms that matter most for relevance and similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fdf35",
   "metadata": {},
   "source": [
    "🗣️ **Instructor Talking Point**: IDF adjusts for the fact that some words are common across all documents — this is critical in improving document relevance in search systems.\n",
    "<br/>\n",
    "<br/>\n",
    "🧠 **Student Talking Point**: Choose a low-IDF and high-IDF term from this output and explain why they behave differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe918e",
   "metadata": {},
   "source": [
    "### 🔹 TF-IDF Weighting\n",
    "\n",
    "**TF-IDF (Term Frequency–Inverse Document Frequency)** scores each term $t$ in document $d$ based on how frequent and how rare it is:\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\left(1 + \\log_{10}(f_{t,d})\\right) \\times \\log_{10} \\left( \\frac{N}{df_t} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f_{t,d}$ is the raw count of term $t$ in document $d$\n",
    "- $df_t$ is the number of documents that contain term $t$\n",
    "- $N$ is the total number of documents in the corpus\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why Use It?\n",
    "\n",
    "- TF-IDF balances **term importance within a document** (TF) against **term commonality across all documents** (IDF).\n",
    "- It **boosts rare, relevant words** while **suppressing frequent, generic words**.\n",
    "- TF-IDF is foundational in:\n",
    "  - Information Retrieval (search engines)\n",
    "  - Document similarity\n",
    "  - Feature engineering for classification or clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### 📘 Example\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "- $f_{\\text{machine}, \\text{Doc1}} = 3$\n",
    "- $df_{\\text{machine}} = 2$\n",
    "- $N = 5$ total documents\n",
    "\n",
    "Then:\n",
    "\n",
    "- TF part: $1 + \\log_{10}(3) \\approx 1 + 0.477 = 1.477$\n",
    "- IDF part: $\\log_{10}(5 / 2) \\approx 0.398$\n",
    "- TF-IDF weight:\n",
    "\n",
    "$$\n",
    "w_{\\text{machine}, \\text{Doc1}} = 1.477 \\times 0.398 \\approx 0.588\n",
    "$$\n",
    "\n",
    "This means \"machine\" is **important within Doc1**, but since it's found in other documents too, the overall weight is **moderated**.\n",
    "\n",
    "TF-IDF creates a **sparse, weighted vector representation** of documents, ready for:\n",
    "- Cosine similarity\n",
    "- Clustering\n",
    "- Search ranking\n",
    "- Input into classical machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33e9308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 TF-IDF Weighted Matrix (Manual Computation):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_18136\\4078578607.py:31: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf_log = 1 + np.where(X_array > 0, np.log10(X_array), 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "advanced",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ai",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "and",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "are",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "deep",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "entropy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "evolving",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "learning",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "machine",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "measures",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "of",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "powerful",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "randomness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "science",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "the",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "f784202d-523e-4b7f-b49e-f19b05fbbf29",
       "rows": [
        [
         "Doc1",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ],
        [
         "Doc2",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ],
        [
         "Doc3",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ],
        [
         "Doc4",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ],
        [
         "Doc5",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advanced</th>\n",
       "      <th>ai</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>deep</th>\n",
       "      <th>entropy</th>\n",
       "      <th>evolving</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>measures</th>\n",
       "      <th>of</th>\n",
       "      <th>powerful</th>\n",
       "      <th>randomness</th>\n",
       "      <th>science</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      advanced     ai    and    are   deep  entropy  evolving     is  \\\n",
       "Doc1     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "Doc2     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "Doc3     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "Doc4     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "Doc5     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "\n",
       "      learning  machine  measures     of  powerful  randomness  science    the  \n",
       "Doc1     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  \n",
       "Doc2     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  \n",
       "Doc3     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  \n",
       "Doc4     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  \n",
       "Doc5     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📘 Example: TF-IDF Weighting (Manual Computation)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus of 5 documents\n",
    "docs = [\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is advanced\",\n",
    "    \"entropy measures randomness\",\n",
    "    \"machine learning and AI are evolving\",\n",
    "    \"the science of machine learning\"\n",
    "]\n",
    "\n",
    "# Total number of documents\n",
    "N = len(docs)\n",
    "\n",
    "# Vectorize (raw term frequencies)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Compute Document Frequencies\n",
    "df = (X_array > 0).sum(axis=0)\n",
    "idf = np.log10(N / df)\n",
    "\n",
    "# Manual TF-IDF: apply (1 + log10(tf)) * idf\n",
    "tf_log = 1 + np.where(X_array > 0, np.log10(X_array), 0)\n",
    "tfidf = tf_log * idf\n",
    "\n",
    "# Create a DataFrame for visual inspection\n",
    "tfidf_df = pd.DataFrame(tfidf, columns=terms, index=[f\"Doc{i+1}\" for i in range(N)])\n",
    "\n",
    "print(\"📊 TF-IDF Weighted Matrix (Manual Computation):\")\n",
    "display(tfidf_df.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051fca38",
   "metadata": {},
   "source": [
    "## Talking Point:\n",
    " in this example, we’re calculating TF-IDF scores manually to figure out which words are most important in each document.\n",
    "\n",
    "We start with 5 simple text documents and count how often each word appears using a tool called CountVectorizer.\n",
    "\n",
    "Then, we calculate something called IDF, which lowers the score for common words and boosts rare, informative ones. So, if a word appears in almost every document, it’s probably not that helpful for identifying what the document is about.\n",
    "\n",
    "Finally, we combine both term frequency and inverse document frequency into a score — that’s our TF-IDF weight. This helps us highlight meaningful words in each doc.\n",
    "\n",
    "We then put everything into a nice table so it’s easier to see which words stand out and in which document.\n",
    "\n",
    "This method is super useful in real-world applications like search engines, spam filters, and even chatbots!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131478c9",
   "metadata": {},
   "source": [
    "🗣️ **Instructor Talking Point**: We combined TF and IDF manually — useful for seeing how each part of the formula shapes the final result.\n",
    "<br/>\n",
    "<br/>\n",
    "🗣️ **Instructor Talking Point**: Document Frequency (DF) counts how many documents contain a specific term, showing how common it is across the corpus.\n",
    "Inverse Document Frequency (IDF) does the opposite—it measures how rare or informative a term is by applying a logarithmic scale to the inverse of DF.\n",
    "So, DF increases with term frequency across documents, while IDF decreases, giving higher weight to rare terms.\n",
    "Together, they balance relevance: DF tells us \"how many use this term,\" while IDF tells us \"how useful is this term for distinguishing documents.\"\n",
    "IDF is critical for reducing noise from overly common words.\n",
    "<br/>\n",
    "<br/>\n",
    "🧠 **Student Talking Point**: \"Pick one row (a document) and explain which term seems most important and why, based on the TF-IDF weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2a938",
   "metadata": {},
   "source": [
    "## Step 2: Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc648f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Load documents from a local folder\n",
    "import os\n",
    "corpus = []\n",
    "for filename in os.listdir('data'):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join('data', filename), 'r', encoding='utf-8') as file:\n",
    "            corpus.append(file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8034f9",
   "metadata": {},
   "source": [
    "## Step 3: Implement a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e744d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine', 'learning', 'is', 'fun!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import List\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.lower().split()\n",
    "\n",
    "# Example\n",
    "tokenize(\"Machine Learning is Fun!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd760f4",
   "metadata": {},
   "source": [
    "## Step 4: Text Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53f7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9bc1c",
   "metadata": {},
   "source": [
    "## Step 5: Build and Test the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23ce7b",
   "metadata": {},
   "source": [
    "\n",
    "Using the six concepts and the preprocessing pipeline above, implement a full pipeline that:\n",
    "- Preprocesses text\n",
    "- Applies vectorization\n",
    "- Computes all six concept metrics\n",
    "- Tests with one phrase query per concept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d8b6b",
   "metadata": {},
   "source": [
    "## Step 6: The Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a255c3d",
   "metadata": {},
   "source": [
    "\n",
    "One team member must push the final notebook to GitHub and send the `.git` URL to the instructor before the end of class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c4ae5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 Learning Objectives\n",
    "- Implement the foundations of **Vector Space Proximity** algorithms using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* – NLP Pipeline and six IR basics techniques implementation + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* – Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - IR Basics & Vector Space Proximity Foundations Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `IRBasics_VectorSpaceProximity.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, Inverted Index and the six concepts.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** (1-2 per concept)\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `IRBasics-VectorSpaceProximity-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055fbe9",
   "metadata": {},
   "source": [
    "## 🔚 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27358c8e",
   "metadata": {},
   "source": [
    "\n",
    "This workshop prepares you for our next session on **Vector Space Proximity** and **Cosine Similarity**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
